<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Welcome file</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h3 id="chapter-1---basic-concepts">Chapter 1 - Basic Concepts</h3>
<h4 id="non-parameteric-models">Non-parameteric models:</h4>
<ul>
<li>Number of parameters grow with the size of the training data</li>
<li>More flexible but computationally intractable for large datasets</li>
<li>Example: KNN with a suitable distance metric - the number of pairs to be considered grows as <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1.064108em; vertical-align: -0.25em;"></span><span class="mord mathit" style="margin-right: 0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141079999999999em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>.</li>
<li>Also the idea of a D-dimensional unit cube with uniform data distributed inside. if we want to “grow” a hypercube that contains 10% of the data, what should its edge-length be?</li>
<li><span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>V</mi><mo>=</mo><msup><mi>e</mi><mn>3</mn></msup>&amp;ThickSpace;<mo>⟹</mo>&amp;ThickSpace;<mi>e</mi><mo>=</mo><mo>(</mo><mn>0.1</mn><msup><mo>)</mo><mfrac><mn>1</mn><mn>3</mn></mfrac></msup><mo>=</mo><mn>0.8</mn></mrow><annotation encoding="application/x-tex">V=e^3 \implies e = (0.1)^\frac{1}{3} = 0.8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.22222em;">V</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 0.838108em; vertical-align: -0.024em;"></span><span class="mord"><span class="mord mathit">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141079999999999em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">⟹</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit">e</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 1.20402em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">0</span><span class="mord">.</span><span class="mord">1</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9540200000000001em;"><span class="" style="top: -3.363em; margin-right: 0.05em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8443142857142858em;"><span class="" style="top: -2.656em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span class="" style="top: -3.2255000000000003em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line mtight" style="border-bottom-width: 0.049em;"></span></span><span class="" style="top: -3.384em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.344em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">8</span></span></span></span></span>, i.e, a the edge of the hypercube has to be 80% of the edgesize of the original unit cube! Hence, in high dimension, nearest neighbours are not truly near anymore.</li>
</ul>
<h4 id="parametric-models">Parametric models:</h4>
<ul>
<li>Fixed number of parameters</li>
<li>Advantage - faster</li>
<li>Disadvantage - make strong assumptions about the data distribution (also known as inductive bias)</li>
<li>Example: Linear regression - assumption that the noise <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi><mo>∼</mo><mi mathvariant="script">N</mi><mo>(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">\epsilon\sim\mathcal{N}(0,1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit">ϵ</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right: 0.14736em;">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span></li>
<li>In other words linear regression models the conditional distribution <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">x</mi><mo separator="true">,</mo><mi>θ</mi><mo>)</mo><mo>=</mo><mi mathvariant="script">N</mi><mo>(</mo><mi>μ</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo><mo separator="true">,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">p(y|\mathbf{x}, \theta) = \mathcal{N}(\mu({\mathbf{x}}), \sigma^2({\mathbf{x}}))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right: 0.03588em;">y</span><span class="mord">∣</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="mord mathit" style="margin-right: 0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 1.064108em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathcal" style="margin-right: 0.14736em;">N</span></span><span class="mopen">(</span><span class="mord mathit">μ</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathbf">x</span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141079999999999em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathbf">x</span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></li>
<li>To go from linear regression to logistic regression
<ul>
<li>The variable y is now modelled as a Bernoulli variable (i.e., with two outcomes instead of continuous outcome space)</li>
<li>The output has to be in the range [0,1]</li>
</ul>
</li>
<li>Logistic Regression: <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi mathvariant="bold">x</mi><mo separator="true">,</mo><mi mathvariant="bold">w</mi><mo>)</mo><mo>=</mo><mi>B</mi><mi>e</mi><mi>r</mi><mo>(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>μ</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">p(y|\mathbf{x}, \mathbf{w}) = Ber(y|\mu(\mathbf{x}))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right: 0.03588em;">y</span><span class="mord">∣</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.16666666666666666em;"></span><span class="mord"><span class="mord mathbf" style="margin-right: 0.01597em;">w</span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit" style="margin-right: 0.05017em;">B</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right: 0.02778em;">r</span><span class="mopen">(</span><span class="mord mathit" style="margin-right: 0.03588em;">y</span><span class="mord">∣</span><span class="mord mathit">μ</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span> where <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>μ</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo><mo>=</mo><mi>p</mi><mo>(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mi mathvariant="normal">∣</mi><mi mathvariant="bold">x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\mu(\mathbf{x}) = p(y=1|\mathbf{x})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit">μ</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit" style="margin-right: 0.03588em;">y</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">1</span><span class="mord">∣</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mclose">)</span></span></span></span></span>, the probability of success. Hence, we model, <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>μ</mi><mo>(</mo><mi mathvariant="bold">x</mi><mo>)</mo><mo>=</mo><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo>(</mo><msup><mi mathvariant="bold">w</mi><mi>T</mi></msup><mi mathvariant="bold">x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\mu(\mathbf{x})=sigmoid(\mathbf{w}^T\mathbf{x})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit">μ</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height: 1.0913309999999998em; vertical-align: -0.25em;"></span><span class="mord mathit">s</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right: 0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">o</span><span class="mord mathit">i</span><span class="mord mathit">d</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right: 0.01597em;">w</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413309999999999em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right: 0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">x</span></span><span class="mclose">)</span></span></span></span></span></li>
</ul>
<h4 id="overfitting">Overfitting:</h4>
<ul>
<li>To explain overfitting, the best example to use is that of KNN. When K=1, the model has zero error on the training set as it perfectly classifies each training datapoint. But test set error is likely to be high. We can slowly increase K, when K=n, we always predict the majority class (underfitting).</li>
<li>K-fold cross validation procedure for model selection</li>
<li>Bias-variance tradeoff: U-shaped curve. Bias (simple model - high bias); Variance (low variance if the model parameters dont change across different subsets of the data - e.g., linear regression line)</li>
<li>No free lunch: Speed-accuracy-complexity-interpretability tradeoffs</li>
</ul>
</div>
</body>

</html>
